"""
í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë²¡í„°í™” (ì„ë² ë”© ìƒì„±)
ì‚¬ìš©ë²•: ê°€ìƒí™˜ê²½ì—ì„œ python background/vector_generator.py
"""

import asyncio
import hashlib
import json
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict

import asyncpg
from openai import AsyncOpenAI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / '.env')

from app.core.config import settings

class VectorGenerator:
    """ë°©íƒˆì¶œ ë°ì´í„° ë²¡í„°í™” ë° ì„ë² ë”© ìƒì„±"""
    
    def __init__(self):
        self.openai_client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.embedding_model = getattr(settings, 'embedding_model', 'text-embedding-ada-002')
        self.embedding_dimension = None  # ë™ì ìœ¼ë¡œ ê²°ì •
        
    async def generate_vectors(self, test_mode: bool = False, test_limit: int = 2):
        """í¬ë¡¤ëŸ¬ ì´í›„ DB ë°ì´í„° ë²¡í„°í™” (ë‹¨ì¼ ëª¨ë“œ)"""
        if test_mode:
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {test_limit}ê°œ ë°ì´í„°ë§Œ ì²˜ë¦¬")
        else:
            print(f"ğŸ“¥ DB ë°ì´í„° ë²¡í„°í™” ì‹œì‘...")
        await self._vectorize_db_data(test_mode, test_limit)
        print("âœ… ë²¡í„° ìƒì„± ì™„ë£Œ!")
    

    async def _vectorize_db_data(self, test_mode: bool = False, test_limit: int = 2):
        """DBì˜ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë²¡í„°í™” (í¬ë¡¤ëŸ¬ ì´í›„ ì‹¤í–‰)"""
        # ë²¡í„° ëª¨ë¸ ì°¨ì› ë™ì  ê°ì§€
        await self._detect_vector_dimension()
        print(f"ğŸ”§ ë²¡í„° ëª¨ë¸: {self.embedding_model} ({self.embedding_dimension}ì°¨ì›)")
        
        # PostgreSQL ì—°ê²°
        conn = await asyncpg.connect(settings.database_url)
        
        try:
            # ë²¡í„°ê°€ ì—†ëŠ” ë°ì´í„° ì¡°íšŒ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì ìš©)
            limit_clause = f"LIMIT {test_limit}" if test_mode else ""
            rows = await conn.fetch(f"""
                SELECT id, name, description, theme, region, sub_region, company,
                       difficulty_level, activity_level, duration_minutes, 
                       price_per_person, group_size_min, group_size_max, rating
                FROM escape_rooms 
                WHERE embedding IS NULL
                ORDER BY id
                {limit_clause}
            """)
            
            if not rows:
                print("ğŸ“‹ ë²¡í„°í™”ê°€ í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
                
            print(f"ğŸ“Š ë²¡í„°í™” ëŒ€ìƒ: {len(rows)}ê°œ")
            
            if test_mode:
                estimated_cost = len(rows) * 0.00015
                estimated_krw = estimated_cost * 1500
                print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ~${estimated_cost:.4f} (â‚©{estimated_krw:.2f}) (í…ŒìŠ¤íŠ¸)")
            else:
                total_cost = len(rows) * 0.00015  # ëŒ€ëµì  ì¶”ì •
                total_krw = total_cost * 1500
                print(f"ğŸ’° ì˜ˆìƒ ì´ ë¹„ìš©: ~${total_cost:.2f} (â‚©{total_krw:.0f})")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batch_size = 1 if test_mode else getattr(settings, 'crawl_batch_size', 10)
            total_cost_actual = 0.0
            
            for i in range(0, len(rows), batch_size):
                batch = rows[i:i + batch_size]
                success_count, batch_cost = await self._process_db_batch(conn, batch, test_mode)
                total_cost_actual += batch_cost
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = min((i + batch_size) / len(rows) * 100, 100)
                krw_actual = total_cost_actual * 1500
                print(f"â³ ë²¡í„°í™” ì§„í–‰ë¥ : {progress:.1f}% ({i + len(batch)}/{len(rows)}) | ëˆ„ì  ë¹„ìš©: ${total_cost_actual:.4f} (â‚©{krw_actual:.2f})")
                
                # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ì¦‰ì‹œ ì¤‘ë‹¨
                if test_mode:
                    krw_test_final = total_cost_actual * 1500
                    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì‹¤ì œ ë¹„ìš©: ${total_cost_actual:.4f} (â‚©{krw_test_final:.2f})")
                    break
                
        finally:
            await conn.close()
    
    
    async def _process_db_batch(self, conn: asyncpg.Connection, batch, test_mode: bool = False):
        """DB ë°ì´í„° ë°°ì¹˜ ë²¡í„°í™” (ìŠ¤ë§ˆíŠ¸ í´ë°± ì „ëµ)"""
        # 1. ëª¨ë“  rowì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±
        descriptions = []
        row_data = []
        
        for row in batch:
            item_dict = dict(row)
            description = self._generate_description_from_dict(item_dict)
            descriptions.append(description)
            row_data.append({
                'id': row['id'],
                'name': row['name'],
                'description': description,
                'processed': False
            })
        
        print(f"    ğŸ”¢ OpenAI ë°°ì¹˜ ë²¡í„° ìƒì„± ì‹œë„: {len(descriptions)}ê°œ")
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ ì‹œë„ (ì‹¤ì œ ë¹„ìš© ì¶”ì )
        self._current_batch_cost = 0.0  # ë°°ì¹˜ ë¹„ìš© ì´ˆê¸°í™”
        success_count = await self._try_batch_vectorization(conn, descriptions, row_data)
        
        # 3. ì‹¤íŒ¨í•œ í•­ëª©ë“¤ ê°œë³„ ì²˜ë¦¬
        failed_items = [item for item in row_data if not item['processed']]
        if failed_items:
            print(f"    ğŸ”„ ì‹¤íŒ¨í•œ {len(failed_items)}ê°œ í•­ëª© ê°œë³„ ì²˜ë¦¬...")
            individual_success = await self._process_failed_items_individually(conn, failed_items)
            success_count += individual_success
        
        total_items = len(batch)
        print(f"    ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_items}ê°œ ì„±ê³µ ({success_count/total_items*100:.1f}%)")
        
        # ì‹¤ì œ API ë¹„ìš© ì‚¬ìš© (ê°œë³„ í˜¸ì¶œì—ì„œ ëˆ„ì ëœ ë¹„ìš©)
        actual_batch_cost = getattr(self, '_current_batch_cost', 0.0)
        
        if test_mode:
            krw_batch_cost = actual_batch_cost * 1500
            print(f"    ğŸ’° ë°°ì¹˜ ì‹¤ì œ ë¹„ìš©: ${actual_batch_cost:.6f} (â‚©{krw_batch_cost:.2f})")
        
        return success_count, actual_batch_cost
    
    async def _try_batch_vectorization(self, conn: asyncpg.Connection, descriptions: List[str], row_data: List[Dict]) -> int:
        """ë°°ì¹˜ ë²¡í„°í™” ì‹œë„"""
        try:
            # OpenAI ë°°ì¹˜ API í˜¸ì¶œ
            vectors = await self._generate_vectors_batch(descriptions)
            
            if len(vectors) != len(descriptions):
                print(f"    âš ï¸ ë²¡í„° ìˆ˜ ë¶ˆì¼ì¹˜: {len(vectors)} vs {len(descriptions)} - ê°œë³„ ì²˜ë¦¬ë¡œ ì „í™˜")
                return 0
            
            # ìœ íš¨í•œ ë²¡í„°ë§Œ í•„í„°ë§ (ë”ë¯¸ ë²¡í„° ì œì™¸)
            valid_updates = []
            for i, (vector, row_info) in enumerate(zip(vectors, row_data)):
                # ë”ë¯¸ ë²¡í„° ì²´í¬ (ëª¨ë‘ 0.0ì¸ ê²½ìš°)
                if not all(v == 0.0 for v in vector):
                    valid_updates.append((vector, row_info['id']))
                    row_info['processed'] = True
                else:
                    print(f"      âš ï¸ ë”ë¯¸ ë²¡í„° ê°ì§€: {row_info['name']} (ID: {row_info['id']})")
            
            if not valid_updates:
                print(f"    âŒ ìœ íš¨í•œ ë²¡í„°ê°€ ì—†ìŒ - ê°œë³„ ì²˜ë¦¬ í•„ìš”")
                return 0
            
            # PostgreSQL ë°°ì¹˜ ì—…ë°ì´íŠ¸ (vector íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
            # Python listë¥¼ PostgreSQL vector í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_updates = []
            for vector, room_id in valid_updates:
                # listë¥¼ "[1,2,3]" í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
                vector_str = '[' + ','.join(map(str, vector)) + ']'
                formatted_updates.append((vector_str, room_id))
            
            await conn.executemany("""
                UPDATE escape_rooms 
                SET embedding = $1::vector 
                WHERE id = $2
            """, formatted_updates)
            
            print(f"    âœ… ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì„±ê³µ: {len(valid_updates)}ê°œ")
            return len(valid_updates)
                
            except Exception as e:
            print(f"    âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return 0
    
    async def _process_failed_items_individually(self, conn: asyncpg.Connection, failed_items: List[Dict]) -> int:
        """ì‹¤íŒ¨í•œ í•­ëª©ë“¤ ê°œë³„ ì²˜ë¦¬"""
        success_count = 0
        final_failures = []
        
        for item in failed_items:
            try:
                # ê°œë³„ ë²¡í„° ìƒì„±
                vector = await self._generate_vector(item['description'])
                
                # None ì²´í¬ (API ìµœì¢… ì‹¤íŒ¨)
                if vector is None:
                    print(f"      âŒ {item['name']}: API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨")
                    final_failures.append({
                        **item,
                        "failure_reason": "api_call_failed"
                    })
                    continue
                
                # ë²¡í„° í’ˆì§ˆ ê²€ì¦ - ì‹¤íŒ¨ì‹œ DB ì—…ë°ì´íŠ¸ í•˜ì§€ ì•ŠìŒ
                if not self._validate_vector_quality(vector):
                    print(f"      âŒ {item['name']}: ìœ íš¨í•˜ì§€ ì•Šì€ ë²¡í„° (ë”ë¯¸/NaN/Inf)")
                    final_failures.append({
                        **item,
                        "failure_reason": "invalid_vector_quality"
                    })
                    # ğŸ¯ í•µì‹¬: DB ì—…ë°ì´íŠ¸ í•˜ì§€ ì•ŠìŒ (embedding = NULL ìœ ì§€)
                    continue
                
                # ìœ íš¨í•œ ë²¡í„°ë§Œ DB ì—…ë°ì´íŠ¸ 
                vector_str = '[' + ','.join(map(str, vector)) + ']'
                await conn.execute("""
                    UPDATE escape_rooms 
                    SET embedding = $1::vector 
                    WHERE id = $2
                """, vector_str, item['id'])
                
                print(f"      âœ… {item['name']} (ID: {item['id']})")
                success_count += 1
                
            except Exception as e:
                print(f"      âŒ {item['name']}: {e}")
                final_failures.append({
                    **item,
                    "failure_reason": str(e)
                })
                continue
        
        # ìµœì¢… ì‹¤íŒ¨í•œ í•­ëª©ë“¤ Dead Letter ì €ì¥
        if final_failures:
            await self._save_failures_to_dead_letter(final_failures)
        
        return success_count
    
    async def _save_failures_to_dead_letter(self, failed_items: List[Dict]):
        """ë²¡í„°í™” ì‹¤íŒ¨ í•­ëª©ë“¤ì„ Dead Letter Queueì— ì €ì¥"""
        try:
            # DLQ ë””ë ‰í† ë¦¬ ìƒì„±
            dlq_dir = Path("data/dead_letters")
            dlq_dir.mkdir(parents=True, exist_ok=True)
            
            # ë‚ ì§œë³„ íŒŒì¼ ìƒì„±
            date_str = datetime.now().strftime("%Y%m%d")
            dlq_file = dlq_dir / f"vector_failures_{date_str}.jsonl"
            
            # ê° ì‹¤íŒ¨ í•­ëª©ì„ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
            with open(dlq_file, 'a', encoding='utf-8') as f:
                for item in failed_items:
                    failed_record = {
                        "timestamp": datetime.now().isoformat(),
                        "error_type": "vectorization_failed",
                        "reason": item.get("failure_reason", "unknown"),
                        "data": {
                            "id": item.get("id"),
                            "name": item.get("name"),
                            "description": item.get("description", "")[:200],  # ì²˜ìŒ 200ìë§Œ
                        },
                        "metadata": {
                            "embedding_model": self.embedding_model,
                            "embedding_dimension": self.embedding_dimension
                        }
                    }
                    json.dump(failed_record, f, ensure_ascii=False)
                    f.write('\n')
            
            print(f"ğŸ’€ ë²¡í„°í™” ì‹¤íŒ¨ {len(failed_items)}ê°œ í•­ëª© Dead Letter ì €ì¥: {dlq_file}")
            
        except Exception as e:
            print(f"âš ï¸ Dead Letter ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _track_api_usage(self, tokens: int, response_time_ms: float, success: bool = True, error_type: str = None):
        """API ì‚¬ìš©ëŸ‰ ì¶”ì  (MVP ë²„ì „)"""
        try:
            # OpenAI ìµœì‹  ê°€ê²© (per 1K tokens)
            # cf. https://platform.openai.com/docs/pricing
            model_costs = {
                'text-embedding-ada-002': 0.00005, 
                'text-embedding-3-small': 0.00001, 
                'text-embedding-3-large': 0.000065,
                'text-embedding-3': 0.000065, 
            }
            
            cost_per_1k = model_costs.get(self.embedding_model, 0.00005)
            cost = (tokens / 1000) * cost_per_1k
            
            # ë¡œê¹… (ë‹¬ëŸ¬ + í•œí™”)
            krw_cost = cost * 1500  # 1ë‹¬ëŸ¬ = 1500ì› ê¸°ì¤€ (ë³´ìˆ˜ì  ì¶”ì •ì¹˜)
            if success:
                print(f"      ğŸ“Š API: {tokens} í† í°, ${cost:.6f} (â‚©{krw_cost:.2f}), {response_time_ms:.0f}ms")
            else:
                print(f"      âŒ API ì‹¤íŒ¨: {error_type}")
                
            return cost
            
        except Exception as e:
            print(f"âš ï¸ API ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _generate_description_from_dict(self, item: Dict) -> str:
        """RAG ìµœì í™”ëœ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""
        # í¬ë¡¤ëŸ¬ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° í•„ë“œëª… ìˆ˜ì •
        duration = item.get('duration_minutes', item.get('duration', 60))
        price = item.get('price_per_person', item.get('price', 0))
        activity_level = item.get('activity_level', 2)
        difficulty_level = item.get('difficulty_level', 3)
        group_min = item.get('group_size_min', 2)
        group_max = item.get('group_size_max', 4)
        rating = item.get('rating', 0)
        
        # RAG: êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ + ìì—°ì–´ ì¡°í•©
        activity_text = {1: "ê±°ì˜ ì—†ìŒ", 2: "ë³´í†µ", 3: "ë§ìŒ"}.get(activity_level, "ë³´í†µ")
        difficulty_text = {1: "ë§¤ìš° ì‰¬ì›€", 2: "ì‰¬ì›€", 3: "ë³´í†µ", 4: "ì–´ë ¤ì›€", 5: "ë§¤ìš° ì–´ë ¤ì›€"}.get(difficulty_level, "ë³´í†µ")
        
        # ğŸ¯ RAG: ê²€ìƒ‰ì„± + ì˜ë¯¸ì  í’ë¶€í•¨
        structured_part = f"""
ë°©íƒˆì¶œ í…Œë§ˆ: {item['name']}
ìš´ì˜ì—…ì²´: {item.get('company', 'ì •ë³´ì—†ìŒ')}
ìœ„ì¹˜: {item['region']} {item.get('sub_region', '')}
ì¥ë¥´: {item['theme']}
ë‚œì´ë„: {difficulty_text} ({difficulty_level}/5)
í™œë™ì„±: {activity_text} ({activity_level}/3)
ì†Œìš”ì‹œê°„: {duration}ë¶„
ì°¸ì—¬ì¸ì›: {group_min}-{group_max}ëª…
1ì¸ë‹¹ ê°€ê²©: {'ê°€ê²© ì •ë³´ ì—†ìŒ' if price == 0 else f'{price:,}ì›'}
í‰ì : {rating}ì 
        """.strip()
        
        # ìì—°ì–´ ë°©íƒˆì¶œ í…Œë§ˆ ì„¤ëª… 
        description = item.get('description', '')
        
        # ğŸš€ ê°œì„ : ê²€ìƒ‰ í‚¤ì›Œë“œ ë³´ê°•
        keywords = self._extract_search_keywords(item)
        
        natural_part = f"""
ì´ ë°©íƒˆì¶œì€ {item['region']} {item.get('sub_region', '')}ì— ìœ„ì¹˜í•œ {item['theme']} í…Œë§ˆì…ë‹ˆë‹¤.
{item.get('company', 'ì—…ì²´')}ì—ì„œ ìš´ì˜í•˜ë©°, {group_min}ëª…ë¶€í„° {group_max}ëª…ê¹Œì§€ ì°¸ì—¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ë‚œì´ë„ëŠ” {difficulty_text}ì´ê³ , í™œë™ì„±ì€ {activity_text} ìˆ˜ì¤€ì…ë‹ˆë‹¤.
ì†Œìš”ì‹œê°„ì€ ì•½ {duration}ë¶„ì´ë©°, 1ì¸ë‹¹ {'ê°€ê²© ì •ë³´ ì—†ìŒ' if price == 0 else f'{price:,}ì›'}ì…ë‹ˆë‹¤.
"""
        
        if description:
            natural_part += f"\nìŠ¤í† ë¦¬: {description}"
        
        if keywords:
            natural_part += f"\nê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(keywords)}"
        
        return f"{structured_part}\n\n{natural_part}".strip()
    
    def _extract_search_keywords(self, item: Dict) -> List[str]:
        """ğŸš€ ì‹¤ë¬´ ê°œì„ : ìœ ì—°í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ğŸ¯ í…Œë§ˆ ê¸°ë°˜ í‚¤ì›Œë“œ
        theme = item.get('theme', '').lower()
        keywords.extend(self._extract_theme_keywords(theme))
        
        # ë‚œì´ë„ ê¸°ë°˜ í‚¤ì›Œë“œ
        difficulty = item.get('difficulty_level', 3)
        keywords.extend(self._extract_difficulty_keywords(difficulty))
        
        # ì¸ì› ê¸°ë°˜ í‚¤ì›Œë“œ  
        group_min = item.get('group_size_min', 2)
        group_max = item.get('group_size_max', 4)
        keywords.extend(self._extract_group_keywords(group_min, group_max))
        
        # ê°€ê²© ê¸°ë°˜ í‚¤ì›Œë“œ
        price = item.get('price_per_person', 0)
        keywords.extend(self._extract_price_keywords(price))
        
        # ì§€ì—­ ê¸°ë°˜ í‚¤ì›Œë“œ (ë™ì  ì¶”ì¶œ)
        region = item.get('region', '')
        sub_region = item.get('sub_region', '')
        keywords.extend(self._extract_location_keywords(region, sub_region))
        
        # ğŸš€ ì‹ ê·œ: ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
        description = item.get('description', '')
        keywords.extend(self._extract_description_keywords(description))
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def _extract_theme_keywords(self, theme: str) -> List[str]:
        """í…Œë§ˆë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìœ ì—°í•œ ë§¤ì¹­)"""
        keywords = []
        
        # ê¸°ë³¸ í•˜ë“œì½”ë”© ë§¤í•‘ (í•µì‹¬ í…Œë§ˆë§Œ)
        base_mappings = {
            'ì¶”ë¦¬': ['ì¶”ë¦¬', 'ìˆ˜ì‚¬', 'íƒì •', 'ë²”ì£„', 'ë¯¸ìŠ¤í„°ë¦¬'],
            'ê³µí¬': ['ê³µí¬', 'ë¬´ì„œìš´', 'ì¢€ë¹„', 'ìœ ë ¹', 'í˜¸ëŸ¬'],
            'ëª¨í—˜': ['ëª¨í—˜', 'íƒí—˜', 'ì–´ë“œë²¤ì²˜', 'ì•¡ì…˜'],
            'ë¡œë§¨ìŠ¤': ['ë¡œë§¨ìŠ¤', 'ì—°ì¸', 'ì»¤í”Œ', 'ì‚¬ë‘'],
            'ì½”ë¯¸ë””': ['ì½”ë¯¸ë””', 'ì›ƒê¸´', 'ì¬ë°ŒëŠ”', 'ìœ ë¨¸'],
            'sf': ['SF', 'ì‚¬ì´íŒŒì´', 'ë¯¸ë˜', 'ìš°ì£¼', 'ë¡œë´‡'],
            'íŒíƒ€ì§€': ['íŒíƒ€ì§€', 'ë§ˆë²•', 'ì¤‘ì„¸', 'ê¸°ì‚¬'],
            'ìŠ¤ë¦´ëŸ¬': ['ìŠ¤ë¦´ëŸ¬', 'ê¸´ì¥ê°', 'ì„œìŠ¤íœìŠ¤']
        }
        
        # ì •í™• ë§¤ì¹­
        for theme_key, theme_words in base_mappings.items():
            if theme_key in theme:
                keywords.extend(theme_words)
        
        # ğŸš€ ìƒˆë¡œìš´ í…Œë§ˆ ìë™ ì²˜ë¦¬ 
        if not keywords:  # ë§¤í•‘ì— ì—†ëŠ” í…Œë§ˆ
            # í…Œë§ˆëª… ìì²´ë¥¼ í‚¤ì›Œë“œë¡œ ì¶”ê°€
            if theme and len(theme) > 1:
                keywords.append(theme)
                
                # ìœ ì‚¬í•œ ì˜ë¯¸ ì¶”ë¡  
                if any(word in theme for word in ['ì¢€ë¹„', 'ê·€ì‹ ', 'ê´´ë¬¼']):
                    keywords.extend(['ê³µí¬', 'ë¬´ì„œìš´'])
                elif any(word in theme for word in ['ì‚¬ë‘', 'ì—°ì• ', 'ë°ì´íŠ¸']):
                    keywords.extend(['ë¡œë§¨ìŠ¤', 'ì»¤í”Œ'])
                elif any(word in theme for word in ['ì›ƒìŒ', 'ì¬ë¯¸', 'ê°œê·¸']):
                    keywords.extend(['ì½”ë¯¸ë””', 'ì¬ë°ŒëŠ”'])
                elif any(word in theme for word in ['ì–´ë ¤ìš´', 'ê·¹í•œ', 'ë„ì „']):
                    keywords.extend(['ê³ ê¸‰', 'ë„ì „ì '])
        
        return keywords
    
    def _extract_difficulty_keywords(self, difficulty: int) -> List[str]:
        """ë‚œì´ë„ í‚¤ì›Œë“œ (ì„¸ë¶„í™”)"""
        if difficulty <= 1:
            return ['ë§¤ìš° ì‰¬ìš´', 'ì…ë¬¸ì', 'ì²˜ìŒ']
        elif difficulty <= 2:
            return ['ì‰¬ìš´', 'ì´ˆë³´ì', 'ì…ë¬¸']
        elif difficulty >= 5:
            return ['ê·¹í•œ', 'ì „ë¬¸ê°€', 'ìµœê³ ë‚œì´ë„']
        elif difficulty >= 4:
            return ['ì–´ë ¤ìš´', 'ê³ ê¸‰', 'ë„ì „ì ']
        else:
            return ['ë³´í†µ', 'ì¼ë°˜']
    
    def _extract_group_keywords(self, group_min: int, group_max: int) -> List[str]:
        """ì¸ì›ìˆ˜ í‚¤ì›Œë“œ"""
        keywords = []
        
        if group_min == 2 and group_max <= 4:
            keywords.extend(['ì»¤í”Œ', 'ì†Œê·œëª¨', 'ë°ì´íŠ¸'])
        elif group_max >= 8:
            keywords.extend(['ëŒ€ê·œëª¨', 'íŒ€ë¹Œë”©', 'ë‹¨ì²´', 'íšŒì‚¬'])
        elif group_max >= 6:
            keywords.extend(['ì¤‘ê·œëª¨', 'ê°€ì¡±', 'ì¹œêµ¬ë“¤'])
        
        # ì •í™•í•œ ì¸ì› í‚¤ì›Œë“œ
        if group_min == group_max:
            keywords.append(f'{group_min}ëª… ì „ìš©')
        
        return keywords
    
    def _extract_price_keywords(self, price: int) -> List[str]:
        """ê°€ê²©ëŒ€ í‚¤ì›Œë“œ"""
        if price < 15000:
            return ['ì €ë ´í•œ', 'ê°€ì„±ë¹„', 'í•™ìƒ']
        elif price < 25000:
            return ['ì ë‹¹í•œ', 'ì¼ë°˜ì ']
        elif price < 40000:
            return ['ì¡°ê¸ˆ ë¹„ì‹¼', 'í€„ë¦¬í‹°']
        else:
            return ['í”„ë¦¬ë¯¸ì—„', 'ê³ ê¸‰', 'íŠ¹ë³„í•œ']
    
    def _extract_location_keywords(self, region: str, sub_region: str) -> List[str]:
        """ì§€ì—­ í‚¤ì›Œë“œ"""
        keywords = []
        
        # ì§€ì—­ ìì²´ ì¶”ê°€
        if region:
            keywords.append(region)
        if sub_region:
            keywords.append(sub_region)
        
        # íŠ¹ë³„í•œ ì§€ì—­ ì†ì„± 
        location_attributes = {
            'ê°•ë‚¨': ['ì ‘ê·¼ì„± ì¢‹ì€', 'ì§€í•˜ì² ', 'ë²ˆí™”ê°€'],
            'í™ëŒ€': ['ëŒ€í•™ê°€', 'ì Šì€', 'í•«í”Œ'],
            'ì‹ ì´Œ': ['ëŒ€í•™ê°€', 'ì Šì€'],
            'ëª…ë™': ['ê´€ê´‘ì§€', 'ì ‘ê·¼ì„±'],
            'ì ì‹¤': ['ë¡¯ë°íƒ€ì›Œ', 'ì‡¼í•‘'],
            'ê°•ë¶': ['ì¡°ìš©í•œ', 'ë™ë„¤'],
            'ëŒ€í•™ë¡œ': ['ê³µì—°', 'ë¬¸í™”']
        }
        
        for location, attrs in location_attributes.items():
            if location in sub_region:
                keywords.extend(attrs)
        
        return keywords
    
    def _extract_description_keywords(self, description: str) -> List[str]:
        """ğŸš€ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (NLP ê¸°ë²•)"""
        if not description or len(description) < 10:
            return []
        
        keywords = []
        desc_lower = description.lower()
        
        # ê°ì •/ë¶„ìœ„ê¸° í‚¤ì›Œë“œ ìë™ ê°ì§€
        emotion_patterns = {
            'ë¬´ì„œìš´': ['ë¬´ì„œ', 'ê³µí¬', 'ë‘ë ¤', 'ì†Œë¦„', 'ë–¨ë¦¼'],
            'ì¬ë°ŒëŠ”': ['ì¬ë¯¸', 'ì›ƒìŒ', 'ì¦ê±°', 'ì‹ ë‚˜', 'ìœ ì¾Œ'],
            'ë¡œë§¨í‹±í•œ': ['ì‚¬ë‘', 'ì—°ì¸', 'ë¡œë§¨', 'ë‹¬ì½¤', 'ì„¤ë ˜'],
            'ê¸´ì¥ê°': ['ê¸´ì¥', 'ìŠ¤ë¦´', 'ì§œë¦¿', 'ì‹¬ì¥', 'ì•„ì°”'],
            'ì‹ ë¹„ë¡œìš´': ['ì‹ ë¹„', 'ë§ˆë²•', 'í™˜ìƒ', 'ì‹ ê¸°', 'ë†€ë¼ìš´']
        }
        
        for emotion, patterns in emotion_patterns.items():
            if any(pattern in desc_lower for pattern in patterns):
                keywords.append(emotion)
        
        # í–‰ë™ í‚¤ì›Œë“œ ê°ì§€
        action_patterns = {
            'ì¶”ë¦¬': ['ì¶”ë¦¬', 'ìˆ˜ì‚¬', 'ë‹¨ì„œ', 'ë²”ì¸', 'ì‚¬ê±´'],
            'íƒˆì¶œ': ['íƒˆì¶œ', 'ë„ë§', 'ë¹ ì ¸ë‚˜', 'ë²—ì–´ë‚˜'],
            'í˜‘ë ¥': ['í˜‘ë ¥', 'íŒ€ì›Œí¬', 'í•¨ê»˜', 'ëª¨ë‘'],
            'ë„ì „': ['ë„ì „', 'ì–´ë ¤ì›€', 'ê·¹ë³µ', 'í•´ê²°']
        }
        
        for action, patterns in action_patterns.items():
            if any(pattern in desc_lower for pattern in patterns):
                keywords.append(action)
        
        return keywords
    
    def _chunk_text_if_needed(self, text: str, max_tokens: int = 8000) -> List[str]:
        """ì‹¤ë¬´ RAG: ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹"""
        # ê°„ë‹¨í•œ í† í° ì¶”ì • (TODO: ì‹¤ì œë¡œëŠ” tiktoken ì‚¬ìš© ê¶Œì¥)
        estimated_tokens = len(text.split()) * 1.3  # í•œêµ­ì–´ëŠ” ì•½ 1.3 í† í°/ë‹¨ì–´
        
        if estimated_tokens <= max_tokens:
            return [text]
        
        # ë¬¸ë‹¨ ê¸°ë°˜ ì²­í‚¹
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph_tokens = len(paragraph.split()) * 1.3
            current_tokens = len(current_chunk.split()) * 1.3
            
            if current_tokens + paragraph_tokens > max_tokens and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += f"\n\n{paragraph}" if current_chunk else paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def _generate_text_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë²¡í„°í™” ë°©ì§€)"""
        # ì •ê·œí™”: ê³µë°±, ì¤„ë°”ê¿ˆ í†µì¼
        normalized = re.sub(r'\s+', ' ', text.strip().lower())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    
    async def _detect_vector_dimension(self):
        """ë²¡í„° ëª¨ë¸ì˜ ì°¨ì›ì„ ë™ì ìœ¼ë¡œ ê°ì§€"""
        if self.embedding_dimension is not None:
            return  # ì´ë¯¸ ê°ì§€ë¨
            
        try:
            # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ ë²¡í„° ìƒì„±í•˜ì—¬ ì°¨ì› í™•ì¸
            response = await self.openai_client.embeddings.create(
                model=self.embedding_model,
                input="test"
            )
            self.embedding_dimension = len(response.data[0].embedding)
            
        except Exception as e:
            print(f"âš ï¸ ë²¡í„° ì°¨ì› ê°ì§€ ì‹¤íŒ¨: {e}")
            # ëª¨ë¸ë³„ ê¸°ë³¸ê°’ ì„¤ì •
            model_dimensions = {
                'text-embedding-ada-002': 1536,
                'text-embedding-3-small': 1536,
                'text-embedding-3-large': 3072,
                'text-embedding-3': 3072,
            }
            self.embedding_dimension = model_dimensions.get(self.embedding_model, 1536)
            print(f"ğŸ”§ ê¸°ë³¸ê°’ ì‚¬ìš©: {self.embedding_dimension}ì°¨ì›")
    
    async def _generate_vector(self, text: str) -> List[float]:
        """OpenAI ë²¡í„° ìƒì„± (ì‹¤ë¬´ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤: ì¬ì‹œë„ + ìƒì„¸ ì—ëŸ¬ í•¸ë“¤ë§)"""
        max_retries = 3
        base_delay = 1
        
        for attempt in range(max_retries):
            try:
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ 
                cleaned_text = self._preprocess_text_for_embedding(text)
                
                # ğŸ“Š MVP ë©”íŠ¸ë¦­: ê°„ë‹¨í•œ í† í° ì¶”ì 
                start_time = time.time()
                estimated_tokens = len(cleaned_text.split()) * 1.3  # í•œêµ­ì–´ ì¶”ì •
                
                response = await self.openai_client.embeddings.create(
                    model=self.embedding_model,
                    input=cleaned_text
                )
                
                # ğŸ“Š MVP ë©”íŠ¸ë¦­: API í˜¸ì¶œ ê¸°ë¡
                response_time_ms = (time.time() - start_time) * 1000
                actual_tokens = response.usage.total_tokens if hasattr(response, 'usage') else int(estimated_tokens)
                
                # ê°„ë‹¨í•œ ë¹„ìš© ê³„ì‚° ë° ë¡œê¹…
                actual_cost = self._track_api_usage(actual_tokens, response_time_ms, success=True)
                
                # ë°°ì¹˜ ë¹„ìš©ì— ì‹¤ì œ ë¹„ìš© ëˆ„ì 
                if hasattr(self, '_current_batch_cost'):
                    self._current_batch_cost += actual_cost
                
                vector = response.data[0].embedding
                
                # ë²¡í„° í’ˆì§ˆ ê²€ì¦ (ì‹¤ë¬´ RAG í•„ìˆ˜)
                if self._validate_vector_quality(vector):
                    return vector
                else:
                    raise ValueError("Invalid vector quality")
                
            except Exception as e:
                error_type = type(e).__name__
                
                # OpenAI íŠ¹í™” ì—ëŸ¬ ì²˜ë¦¬
                if "rate_limit" in str(e).lower():
                    delay = base_delay * (2 ** attempt) + 2  # ë ˆì´íŠ¸ ë¦¬ë°‹ì‹œ ë” ê¸´ ëŒ€ê¸°
                    print(f"ğŸ”„ Rate limit ì¬ì‹œë„ {attempt+1}/{max_retries} (ëŒ€ê¸°: {delay}ì´ˆ)")
                    await asyncio.sleep(delay)
                    continue
                elif "context_length" in str(e).lower():
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ì²­í‚¹
                    print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ - ì²­í‚¹ ì‹œë„")
                    chunks = self._chunk_text_if_needed(text, max_tokens=6000)
                    if len(chunks) > 1:
                        # ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì‚¬ìš© (ê°„ë‹¨í•œ í´ë°±)
                        return await self._generate_vector(chunks[0])
                elif attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    print(f"âš ï¸ ë²¡í„° ìƒì„± ì˜¤ë¥˜ ({error_type}): {e}")
                    print(f"ğŸ”„ ì¬ì‹œë„ {attempt+1}/{max_retries} (ëŒ€ê¸°: {delay}ì´ˆ)")
                    await asyncio.sleep(delay)
                    continue
                else:
                    print(f"âŒ ìµœì¢… ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({error_type}): {e}")
                    # ì‹¤íŒ¨ ì¶”ì 
                    self._track_api_usage(0, 0, success=False, error_type=error_type)
        
        # ğŸ¯ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ì‹œ None ë°˜í™˜ (ë”ë¯¸ ë²¡í„° ëŒ€ì‹ )
        # í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ None ì²´í¬ í›„ DB ì—…ë°ì´íŠ¸ ìŠ¤í‚µ
        return None
    
    def _preprocess_text_for_embedding(self, text: str) -> str:
        """ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì‹¤ë¬´ RAG ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)"""
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = text.strip()
        
        # 2. ê³¼ë„í•œ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬  
        text = re.sub(r'\n{3,}', '\n\n', text)  # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r' {2,}', ' ', text)      # 2ê°œ ì´ìƒ ê³µë°± â†’ 1ê°œ
        
        # 3. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ê²€ìƒ‰ ë°©í•´ ìš”ì†Œ)
        text = re.sub(r'[^\w\sê°€-í£.,!?():\-]', '', text)
        
        # 4. ê¸¸ì´ ì œí•œ (í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        if len(text) > 30000:  # ì•½ 8000 í† í° ì¶”ì •
            text = text[:30000] + "..."
            
        return text
    
    def _validate_vector_quality(self, vector: List[float]) -> bool:
        """ë²¡í„° í’ˆì§ˆ ê²€ì¦"""
        if not vector:
            return False
            
        # 1. ëª¨ë‘ 0ì¸ ë²¡í„° (ë”ë¯¸) ì²´í¬
        if all(v == 0.0 for v in vector):
            return False
            
        # 2. NaN/Inf ì²´í¬
        if any(not (-1e10 < v < 1e10) for v in vector):
            return False
            
        # 3. ë²¡í„° ë…¸ë¦„ ì²´í¬ (ë„ˆë¬´ ì‘ê±°ë‚˜ í° ë²¡í„°)
        norm = sum(v * v for v in vector) ** 0.5
        if norm < 1e-6 or norm > 10:
            return False
            
        return True
    
    async def _generate_vectors_batch(self, texts: List[str]) -> List[List[float]]:
        """OpenAI ë°°ì¹˜ ë²¡í„° ìƒì„±"""
        try:
            # OpenAI APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 2048ê°œ ì…ë ¥ ì§€ì›
            max_batch_size = min(len(texts), 100)  # ì•ˆì „í•˜ê²Œ 100ê°œì”©
            
            if len(texts) <= max_batch_size:
                # í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
                start_time = time.time()
                response = await self.openai_client.embeddings.create(
                    model=self.embedding_model,
                    input=texts
                )
                
                # ì‹¤ì œ í† í° ìˆ˜ ì¶”ì 
                if hasattr(response, 'usage') and response.usage:
                    response_time_ms = (time.time() - start_time) * 1000
                    actual_cost = self._track_api_usage(response.usage.total_tokens, response_time_ms, success=True)
                    self._current_batch_cost = getattr(self, '_current_batch_cost', 0.0) + actual_cost
                
                return [item.embedding for item in response.data]
            else:
                # ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
                all_vectors = []
                for i in range(0, len(texts), max_batch_size):
                    chunk = texts[i:i + max_batch_size]
                    start_time = time.time()
            response = await self.openai_client.embeddings.create(
                model=self.embedding_model,
                        input=chunk
                    )
                    
                    # ì‹¤ì œ í† í° ìˆ˜ ì¶”ì 
                    if hasattr(response, 'usage') and response.usage:
                        response_time_ms = (time.time() - start_time) * 1000
                        actual_cost = self._track_api_usage(response.usage.total_tokens, response_time_ms, success=True)
                        self._current_batch_cost = getattr(self, '_current_batch_cost', 0.0) + actual_cost
                    
                    chunk_vectors = [item.embedding for item in response.data]
                    all_vectors.extend(chunk_vectors)
                
                return all_vectors
            
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            # ğŸ¯ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ë”ë¯¸ ë²¡í„° ëŒ€ì‹ )
            # í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ "ë²¡í„° ìˆ˜ ë¶ˆì¼ì¹˜"ë¡œ ì²˜ë¦¬ë¨
            return []
    

    

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    generator = VectorGenerator()
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€ í™•ì¸ 
    test_mode = "--test" in sys.argv
    
    if test_mode:
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘")
        await generator.generate_vectors(test_mode=True, test_limit=2)
    else:
        print("ğŸ”§ DB ë°ì´í„° ë²¡í„°í™” ëª¨ë“œ (ì „ì²´)")
        print("ğŸ’¡ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´: python vector_generator.py --test")
        await generator.generate_vectors()

if __name__ == "__main__":
    asyncio.run(main())