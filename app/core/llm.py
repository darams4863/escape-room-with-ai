"""LLM ë° ì„ë² ë”© ì„œë¹„ìŠ¤ (ê³µí†µ ê¸°ëŠ¥)"""

import time
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from .config import settings
from .logger import logger
from .monitor import track_api_call, track_performance


class LLM:
    """LLM ë° ì„ë² ë”© ì„œë¹„ìŠ¤ (ìƒíƒœ ìœ ì§€ í•„ìš”)"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY
        )
        self.embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)
        
        # ê²½í—˜ ë“±ê¸‰ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.chat_prompt = PromptTemplate(
            input_variables=["conversation_history", "user_message", "user_level", "user_preferences"],
            template="""
ë‹¹ì‹ ì€ **ì¹œê·¼í•œ AI ì±—ë´‡ì´ë©´ì„œ ë™ì‹œì— ë°©íƒˆì¶œ ì „ë¬¸ ë§¤ë‹ˆì €**ì…ë‹ˆë‹¤.

## ğŸ¯ ë‹¹ì‹ ì˜ ì—­í• 
1. **ì¼ë°˜ ì±—ë´‡**: ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©° ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ì¹œê·¼í•˜ê²Œ ì‘ë‹µ
2. **ë°©íƒˆì¶œ ì „ë¬¸ê°€**: ì „êµ­ ë°©íƒˆì¶œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì¶”ì²œê³¼ ì¡°ì–¸ ì œê³µ

## ğŸ—ºï¸ ë³´ìœ  ì •ë³´
- **ì „êµ­ ë°©íƒˆì¶œ ë°ì´í„°ë² ì´ìŠ¤**: ì„œìš¸/ì¸ì²œ/ê²½ê¸°/ì¶©ë‚¨/ì¶©ë¶/ì œì£¼ë„ ë“± ì „êµ­ ë°©íƒˆì¶œ ì •ë³´
- **ë‹¤ì–‘í•œ í…Œë§ˆ**: ì¶”ë¦¬, ê³µí¬, ë¡œë§¨ìŠ¤, íŒíƒ€ì§€, SF, ì—­ì‚¬, ì•¡ì…˜ ë“± ëª¨ë“  ì¥ë¥´
- **ìƒì„¸ ì •ë³´**: ë‚œì´ë„, ê°€ê²©, ì¸ì›ìˆ˜, ì§€ì—­, ìš´ì˜ì‹œê°„, ì˜ˆì•½ ë°©ë²•, í™œë™ì„± ë“±

## ğŸ‘¤ ì‚¬ìš©ì ì •ë³´
- ê²½í—˜ ë“±ê¸‰: {user_level}
- ì„ í˜¸ì‚¬í•­: {user_preferences}

## ğŸ’¬ ëŒ€í™” ê¸°ë¡
{conversation_history}

## ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€
{user_message}

## ğŸ­ ì‘ë‹µ ê°€ì´ë“œë¼ì¸

### 1. ëŒ€í™” ìŠ¤íƒ€ì¼
- **ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤**ìœ¼ë¡œ ëŒ€í™”
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸ˜ŠğŸ¯ğŸ’¡ğŸ”¥)
- ì‚¬ìš©ìì˜ ê°ì •ì— ê³µê°í•˜ê³  ê²©ë ¤

### 2. ë°©íƒˆì¶œ ê´€ë ¨ ì§ˆë¬¸ ì‹œ
- ê²½í—˜ ë“±ê¸‰ì— ë§ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µ:
  - **ë°©ìƒì•„/ë°©ë¦°ì´**: ì¹œì ˆí•œ ê°€ì´ë“œ í†¤ "ì²˜ìŒì´ì‹œë¼ë©´ ì´ ë°©ì´ ë”± ì¢‹ì•„ìš”!"
  - **ë°©ì†Œë…„/ë°©ì–´ë¥¸**: ë™ë£Œ ê²Œì´ë¨¸ í†¤ "ì´ ì •ë„ëŠ” ê°€ë³ê²Œ ì¦ê¸°ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”"
  - **ë°©ì‹ /ë°©ì¥ë¡œ**: ì¡´ì¤‘ + ë„ì „ í†¤ "ì´ê±´ ê³ ì¸ë¬¼ë„ í˜€ë¥¼ ë‚´ë‘ë¥¸ë‹¤ëŠ” ë°©ì´ì—ìš”"

### 3. ì¼ë°˜ ëŒ€í™” ì‹œ
- ë°©íƒˆì¶œê³¼ ê´€ë ¨ ì—†ëŠ” ì£¼ì œë„ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”
- í•„ìš”ì‹œ ë°©íƒˆì¶œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
- ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì— ë§ì¶° ëŒ€í™”

### 4. ì¶”ì²œ ì‹œ ê³ ë ¤ì‚¬í•­
- ì‚¬ìš©ì ì„ í˜¸ì‚¬í•­ (ì§€ì—­, í…Œë§ˆ, ë‚œì´ë„, ê°€ê²©, ì¸ì›ìˆ˜)
- ê²½í—˜ ë“±ê¸‰ì— ë§ëŠ” ë‚œì´ë„
- ê³„ì ˆ, ì‹œê°„ëŒ€, íŠ¹ë³„í•œ ìƒí™© ê³ ë ¤

## ğŸ“‹ ì‘ë‹µ í˜•ì‹
- **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ì‚¬ìš©ìì™€ì˜ ì¹œê·¼í•œ ëŒ€í™”
- **êµ¬ì²´ì  ì •ë³´**: í•„ìš”ì‹œ ë°©íƒˆì¶œ ìƒì„¸ ì •ë³´ ì œê³µ
- **ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ**: ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ì¶”ì²œ ìœ ë„

ì‘ë‹µí•´ì£¼ì„¸ìš”:
"""
        )
        
        # LLMChain ëŒ€ì‹  ìµœì‹  ë°©ì‹ ì‚¬ìš©
        self.chain = self.chat_prompt | self.llm
    
    @track_performance("llm_generate_response")
    async def generate_response(
        self, 
        conversation_history, 
        user_level: str = "ë°©ìƒì•„",
        user_preferences: dict = None,
        custom_prompt: str = None
    ) -> str:
        """LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        try:
            start_time = time.time()
            
            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
            if custom_prompt:
                response = await self.llm.ainvoke(custom_prompt)
            else:
                # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                history_text = ""
                for msg in conversation_history:
                    role = "ì‚¬ìš©ì" if msg.role == "user" else "AI"
                    history_text += f"{role}: {msg.content}\n"
                
                # ì‚¬ìš©ì ì„ í˜¸ì‚¬í•­ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                prefs_text = ""
                if user_preferences:
                    prefs_list = []
                    for key, value in user_preferences.items():
                        if value:
                            prefs_list.append(f"{key}: {value}")
                    prefs_text = ", ".join(prefs_list) if prefs_list else "ì—†ìŒ"
                else:
                    prefs_text = "ì—†ìŒ"
                
                # LLM í˜¸ì¶œ
                response = await self.chain.ainvoke({
                    "conversation_history": history_text,
                    "user_message": conversation_history[-1].content if conversation_history else "",
                    "user_level": user_level,
                    "user_preferences": prefs_text
                })
            
            response_time = time.time() - start_time
            
            # ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ê³„ì‚°
            llm_output = response.response_metadata
            prompt_tokens = llm_output.get('token_usage', {}).get('prompt_tokens', 0)
            completion_tokens = llm_output.get('token_usage', {}).get('completion_tokens', 0)
            
            # GPT-4o-mini ê°€ê²© (2025ë…„ 9ì›” 20ì¼ ê¸°ì¤€)
            input_cost = (prompt_tokens / 1000000) * 0.15  # $0.15 per 1M tokens
            output_cost = (completion_tokens / 1000000) * 0.60  # $0.60 per 1M tokens
            total_cost = input_cost + output_cost
            
            # í•œêµ­ ì›í™” í™˜ìœ¨ ê³„ì‚° (1 USD = 1500 KRW)
            total_cost_krw = total_cost * 1500
            logger.info(f"LLM ë¹„ìš©: ${total_cost:.6f} (â‚©{total_cost_krw:.2f}) - Input: {prompt_tokens} tokens, Output: {completion_tokens} tokens")
            
            # API í˜¸ì¶œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (í•œ ë²ˆë§Œ)
            track_api_call(
                service="openai",
                endpoint="generate_response",
                status_code=200,
                duration_seconds=response_time,
                model="gpt-4o-mini",
                cost_usd=total_cost
            )
            
            return response.content
            
        except Exception as e:
            logger.error(f"LLM response generation error: {e}")
            # API í˜¸ì¶œ ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            track_api_call(model="gpt-4o-mini", success=False)
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    @track_performance("llm_create_embedding")
    async def create_embedding(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            start_time = time.time()
            response = await self.embeddings.aembed_query(text)
            response_time = time.time() - start_time
            
            # ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ê³„ì‚°
            # text-embedding-ada-002: $0.0001 per 1K tokens (2025ë…„ 9ì›” 20ì¼ ê¸°ì¤€)
            # ì„ë² ë”©ì€ responseê°€ listì´ë¯€ë¡œ í† í° ìˆ˜ë¥¼ ì¶”ì •
            estimated_tokens = len(text.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
            cost_usd = (estimated_tokens / 1000) * 0.0001
            
            # í•œêµ­ ì›í™” í™˜ìœ¨ ê³„ì‚° (1 USD = 1500 KRW)
            total_cost_krw = cost_usd * 1500
            logger.info(f"ì„ë² ë”© ìƒì„± ë¹„ìš©: ${cost_usd:.6f} (â‚©{total_cost_krw:.2f}) - ì¶”ì • í† í°: {estimated_tokens}")
            
            # API í˜¸ì¶œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            track_api_call(
                service="openai",
                endpoint="create_embedding",
                status_code=200,
                duration_seconds=response_time,
                model="text-embedding-ada-002",
                cost_usd=cost_usd
            )
            return response
        except Exception as e:
            logger.error(f"Embedding creation error: {e}")
            return []


llm = LLM()
