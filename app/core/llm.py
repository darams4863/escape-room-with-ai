"""LLM ë° ì„ë² ë”© ì„œë¹„ìŠ¤ (ê³µí†µ ê¸°ëŠ¥)"""

from typing import List

from langchain.schema import BaseMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from .config import settings
from .logger import logger


class LLMService:
    """LLM ì„œë¹„ìŠ¤ ë ˆì´ì–´"""
    
    def __init__(self, provider: str = "openai"):
        self.provider = provider
        self._setup_llm()
        self._setup_embeddings()
    
    def _setup_llm(self):
        """LLM ì„¤ì •"""
        if self.provider == "openai":
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.7,
                openai_api_key=settings.OPENAI_API_KEY
            )
        # NOTE: í™•ì¥ ê°€ëŠ¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ ê³ ë ¤
        # elif self.provider == "anthropic":
        #     self.llm = ChatAnthropic(...)
        # elif self.provider == "google":
        #     self.llm = ChatGoogleGenerativeAI(...)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def _setup_embeddings(self):
        """ì„ë² ë”© ì„¤ì • (í™•ì¥ ê°€ëŠ¥)"""
        if self.provider == "openai":
            self.embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)
        # elif self.provider == "cohere":
        #     self.embeddings = CohereEmbeddings(...)
        else:
            raise ValueError(f"Unsupported embedding provider: {self.provider}")
    
    async def _generate_response(self, prompt: str) -> str:
        """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± (ë‚´ë¶€ìš©)"""
        try:
            response = await self.llm.ainvoke(prompt)
            return response.content
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            raise
    
    async def generate_with_messages(self, messages: List[BaseMessage]) -> str:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„± (LangChain í‘œì¤€)"""
        try:
            response = await self.llm.agenerate([messages])
            return response.generations[0][0].text
        except Exception as e:
            logger.error(f"LLM generation with messages error: {e}")
            raise
    
    async def generate_with_messages_and_usage(self, messages: List[BaseMessage]) -> tuple[str, dict]:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„± + í† í° ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        try:
            response = await self.llm.agenerate([messages])
            text = response.generations[0][0].text
            usage = response.llm_output.get('token_usage', {}) if response.llm_output else {}
            return text, usage
        except Exception as e:
            logger.error(f"LLM generation with usage error: {e}")
            raise
    
    async def create_embedding(self, text: str) -> List[float]:
        """ì„ë² ë”© ìƒì„±"""
        try:
            return await self.embeddings.aembed_query(text)
        except Exception as e:
            logger.error(f"Embedding creation error: {e}")
            raise
    
    async def generate_chat_response(
        self, 
        conversation_history, 
        user_level: str = "ë°©ìƒì•„",
        user_preferences: dict = None
    ) -> str:
        """ì¼ë°˜ ëŒ€í™”ìš© ì‘ë‹µ ìƒì„±"""
        try:
            # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ìµœê·¼ 3ê°œë§Œ)
            history_text = ""
            recent_messages = conversation_history[-3:] if len(conversation_history) > 3 else conversation_history
            
            for msg in recent_messages:
                role = "ì‚¬ìš©ì" if msg.role == "user" else "AI"
                history_text += f"{role}: {msg.content}\n"
            
            # ì‚¬ìš©ì ì„ í˜¸ì‚¬í•­ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            prefs_text = ""
            if user_preferences:
                prefs_list = []
                for key, value in user_preferences.items():
                    if value:
                        prefs_list.append(f"{key}: {value}")
                prefs_text = ", ".join(prefs_list) if prefs_list else "ì—†ìŒ"
            else:
                prefs_text = "ì—†ìŒ"
            
            # ì¼ë°˜ ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸
            chat_prompt = f"""ë‹¹ì‹ ì€ **ì¹œê·¼í•œ AI ì±—ë´‡ì´ë©´ì„œ ë™ì‹œì— ë°©íƒˆì¶œ ì „ë¬¸ ë§¤ë‹ˆì €**ì…ë‹ˆë‹¤.

## ğŸ¯ ë‹¹ì‹ ì˜ ì—­í• 
1. **ì¼ë°˜ ì±—ë´‡**: ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©° ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ì¹œê·¼í•˜ê²Œ ì‘ë‹µ
2. **ë°©íƒˆì¶œ ì „ë¬¸ê°€**: ì „êµ­ ë°©íƒˆì¶œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì¶”ì²œê³¼ ì¡°ì–¸ ì œê³µ

## ğŸ‘¤ ì‚¬ìš©ì ì •ë³´
- ê²½í—˜ ë“±ê¸‰: {user_level}
- ì„ í˜¸ì‚¬í•­: {prefs_text}

## ğŸ’¬ ëŒ€í™” ê¸°ë¡ (ì°¸ê³ ìš©)
{history_text}

## ğŸ­ ì‘ë‹µ ê°€ì´ë“œë¼ì¸
- **ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤**ìœ¼ë¡œ ëŒ€í™”
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸ˜ŠğŸ¯ğŸ’¡ğŸ”¥)
- **ì˜¤ëŠ˜ì˜ ëŒ€í™” ë‚´ìš©ì—ë§Œ ì§‘ì¤‘**í•˜ì—¬ ì‘ë‹µ
- ì´ì „ ëŒ€í™”ëŠ” **ì»¨í…ìŠ¤íŠ¸ë¡œë§Œ ì°¸ê³ ** (ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ)
- **í˜„ì¬ ë©”ì‹œì§€ì˜ ì£¼ì œì—ë§Œ ì§‘ì¤‘**

ì‘ë‹µí•´ì£¼ì„¸ìš”:
"""
            
            return await self._generate_response(chat_prompt)
            
        except Exception as e:
            logger.error(f"Chat response generation error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    
# ì „ì—­ LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
llm_service = LLMService()

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
llm = llm_service
