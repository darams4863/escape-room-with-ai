"""ì±— ë©”ì„¸ì§€ ì˜ë„ ë¶„ì„ ì „ë‹´ ì„œë¹„ìŠ¤ (LLM + DB Fallback)"""

import json
import re
from ..utils.time import now_korea_iso
from typing import Dict, Any, List
from ..core.logger import logger
from ..repositories.escape_room_repository import get_intent_patterns_from_db
from ..core.llm import llm
from langchain.schema import HumanMessage
from ..core.config import settings


# í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ë¶„ê¸° í•¨ìˆ˜
def _build_prompt_v1_2(user_message: str) -> str:
    """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ v1.2"""
    return f"""
[VERSION INFO]
prompt_version: intent.v1.2
schema_version: entities.v1.2

ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}

ì˜ë„ë¥¼ ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•˜ê³  JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”:
1. "recommendation" - ë°©íƒˆì¶œ ì¶”ì²œ ìš”ì²­
2. "question" - ë°©íƒˆì¶œ ê´€ë ¨ ì§ˆë¬¸  
3. "general_chat" - ì¼ë°˜ì ì¸ ëŒ€í™”
4. "preference_check" - ì„ í˜¸ë„ ì²´í¬

ì¶”ê°€ë¡œ ë‹¤ìŒ ì •ë³´ë„ í¬í•¨í•´ì£¼ì„¸ìš”:
- confidence: ì˜ë„ íŒŒì•… ì‹ ë¢°ë„ (0.0-1.0)
- entities: ì¶”ì¶œëœ ì—”í‹°í‹° (preferred_region, excluded_region, preferred_themes, excluded_themes, group_size, relationship, difficulty, activity_level, duration_minutes, price_per_person, group_size_min, group_size_max, company, rating)
- reasoning: ì˜ë„ íŒŒì•… ê·¼ê±°

**ì¤‘ìš”**: 
- "ê³µí¬ í…Œë§ˆëŠ” ì•ˆë¼", "í˜¸ëŸ¬ ì‹«ì–´" ê°™ì€ í‘œí˜„ì€ ì œì™¸ í…Œë§ˆ(excluded_themes)ë¡œ ë¶„ë¥˜
- ì„ í˜¸í•˜ëŠ” í…Œë§ˆ(preferred_themes)ì™€ ì œì™¸í•˜ëŠ” í…Œë§ˆ(excluded_themes)ë¥¼ êµ¬ë¶„í•´ì„œ ì¶”ì¶œ
    - ì˜ˆ: ì•„ë˜ í…Œë§ˆ ì˜ˆì‹œì—ì„œ ê³µí¬ëŠ” excluded_themesì— ë“¤ì–´ê°€ê³ , ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ preferred_themesì— ë“¤ì–´ê°„ë‹¤
- preferred_region, excluded_regionë„ í•˜ë‹¨ ì§€ì—­ ì˜ˆì‹œì—ì„œ ì •í™•í•˜ê²Œ ì¶”ì¶œ
    - ì˜ˆ: ì‚¬ìš©ì ìš”ì²­ì´ "ê°•ë‚¨ì—ì„œ ì¶”ë¦¬ í…Œë§ˆë¡œ ì¶”ì²œí•´ì¤˜" ì¼ ë•Œ, preferred_regionì€ "ê°•ë‚¨"ì´ê³ , excluded_regionì€ []ì´ë‹¤

**í…Œë§ˆì™€ ì§€ì—­ ì˜ˆì‹œ**:
í…Œë§ˆ: 'ìŠ¤ë¦´ëŸ¬', 'ê¸°íƒ€', 'íŒíƒ€ì§€', 'ì¶”ë¦¬', 'í˜¸ëŸ¬/ê³µí¬', 'ì ì…', 'ëª¨í—˜/íƒí—˜', 'ê°ì„±', 'ì½”ë¯¸ë””', 'ë“œë¼ë§ˆ', 'ë²”ì£„', 'ë¯¸ìŠ¤í„°ë¦¬', 'SF', '19ê¸ˆ', 'ì•¡ì…˜', 'ì—­ì‚¬', 'ë¡œë§¨ìŠ¤', 'ì•„ì´', 'íƒ€ì„ì–´íƒ'
ì§€ì—­: 'ì„œìš¸', 'ê°•ë‚¨', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ì‹ ë¦¼', 'ê±´ëŒ€', 'êµ¬ë¡œêµ¬', 'ë…¸ì›êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'í™ëŒ€', 'ì‹ ì´Œ', 'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì ì‹¤', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ëŒ€í•™ë¡œ', 'ì¤‘êµ¬', 'ê²½ê¸°', 'ê³ ì–‘', 'ê´‘ì£¼', 'êµ¬ë¦¬', 'êµ°í¬', 'ê¹€í¬', 'ë™íƒ„', 'ë¶€ì²œ', 'ì„±ë‚¨', 'ìˆ˜ì›', 'ì‹œí¥', 'ì•ˆì‚°', 'ì•ˆì–‘', 'ìš©ì¸', 'ì˜ì •ë¶€', 'ì´ì²œ', 'ì¼ì‚°', 'í‰íƒ', 'í•˜ë‚¨', 'í™”ì„±', 'ì¸ì²œ', 'ë‚¨ë™êµ¬', 'ë¯¸ì¶”í™€êµ¬', 'ë¶€í‰êµ¬', 'ì—°ìˆ˜êµ¬', 'ì „ë¶', 'êµ°ì‚°', 'ìµì‚°', 'ì „ì£¼', 'ì¶©ë‚¨', 'ë‹¹ì§„', 'ì²œì•ˆ', 'ê²½ë‚¨', 'ì–‘ì‚°', 'ì§„ì£¼', 'ì°½ì›', 'ê°•ì›', 'ê°•ë¦‰', 'ì›ì£¼', 'ì¶˜ì²œ', 'ì œì£¼', 'ì„œê·€í¬ì‹œ', 'ì œì£¼ì‹œ', 'ì¶©ë¶', 'ì²­ì£¼', 'ì „ë‚¨', 'ëª©í¬', 'ìˆœì²œ', 'ì—¬ìˆ˜', 'ê²½ë¶', 'ê²½ì£¼', 'êµ¬ë¯¸', 'ì˜ì£¼', 'í¬í•­'

**ì—”í‹°í‹° ì¶”ì¶œ ì˜ˆì‹œ**:
- "ê°•ë‚¨ì—ì„œ ì¶”ë¦¬ í…Œë§ˆë¡œ ì¶”ì²œí•´ì¤˜" â†’ {{"preferred_region": ["ê°•ë‚¨"], "preferred_themes": ["ì¶”ë¦¬"]}}
- "ê³µí¬ í…Œë§ˆëŠ” ì ˆëŒ€ ì•ˆë¼" â†’ {{"excluded_themes": ["ê³µí¬"]}}
- "2ëª…ì´ í•  ìˆ˜ ìˆëŠ” ê±°" â†’ {{"group_size": 2}}
- "ë‚¨ìì¹œêµ¬ë‘ í• ë§Œí•œ" â†’ {{"relationship": "ì»¤í”Œ", "preferred_themes": ["ë¡œë§¨ìŠ¤", "ë“œë¼ë§ˆ"]}}

**ì‘ë‹µ í˜•ì‹**:
JSON ìµœìƒìœ„ì— "_meta" ê°ì²´ë¥¼ í¬í•¨í•˜ì—¬ ë²„ì „ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.

JSON ì‘ë‹µ:
"""

def _build_prompt_v1_3(user_message: str) -> str:
    """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ v1.3 (ë” êµ¬ì²´ì ì¸ ì˜ˆì‹œ)"""
    return f"""
[VERSION INFO]
prompt_version: intent.v1.3
schema_version: entities.v1.3

ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}

ì˜ë„ë¥¼ ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•˜ê³  JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”:
1. "recommendation" - ë°©íƒˆì¶œ ì¶”ì²œ ìš”ì²­
2. "question" - ë°©íƒˆì¶œ ê´€ë ¨ ì§ˆë¬¸  
3. "general_chat" - ì¼ë°˜ì ì¸ ëŒ€í™”
4. "preference_check" - ì„ í˜¸ë„ ì²´í¬

ì¶”ê°€ë¡œ ë‹¤ìŒ ì •ë³´ë„ í¬í•¨í•´ì£¼ì„¸ìš”:
- confidence: ì˜ë„ íŒŒì•… ì‹ ë¢°ë„ (0.0-1.0)
- entities: ì¶”ì¶œëœ ì—”í‹°í‹° (preferred_region, excluded_region, preferred_themes, excluded_themes, group_size, relationship, difficulty, activity_level, duration_minutes, price_per_person, group_size_min, group_size_max, company, rating)
- reasoning: ì˜ë„ íŒŒì•… ê·¼ê±°

**ì¤‘ìš”**: 
- "ê³µí¬ í…Œë§ˆëŠ” ì•ˆë¼", "í˜¸ëŸ¬ ì‹«ì–´", "ì ˆëŒ€ ì•ˆë¼" ê°™ì€ í‘œí˜„ì€ ì œì™¸ í…Œë§ˆ(excluded_themes)ë¡œ ë¶„ë¥˜
- ì„ í˜¸í•˜ëŠ” í…Œë§ˆ(preferred_themes)ì™€ ì œì™¸í•˜ëŠ” í…Œë§ˆ(excluded_themes)ë¥¼ êµ¬ë¶„í•´ì„œ ì¶”ì¶œ
- ì§€ì—­ì€ ì •í™•í•˜ê²Œ ì¶”ì¶œ

**í…Œë§ˆì™€ ì§€ì—­ ì˜ˆì‹œ**:
í…Œë§ˆ: 'ìŠ¤ë¦´ëŸ¬', 'ê¸°íƒ€', 'íŒíƒ€ì§€', 'ì¶”ë¦¬', 'í˜¸ëŸ¬/ê³µí¬', 'ì ì…', 'ëª¨í—˜/íƒí—˜', 'ê°ì„±', 'ì½”ë¯¸ë””', 'ë“œë¼ë§ˆ', 'ë²”ì£„', 'ë¯¸ìŠ¤í„°ë¦¬', 'SF', '19ê¸ˆ', 'ì•¡ì…˜', 'ì—­ì‚¬', 'ë¡œë§¨ìŠ¤', 'ì•„ì´', 'íƒ€ì„ì–´íƒ'
ì§€ì—­: 'ì„œìš¸', 'ê°•ë‚¨', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ì‹ ë¦¼', 'ê±´ëŒ€', 'êµ¬ë¡œêµ¬', 'ë…¸ì›êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'í™ëŒ€', 'ì‹ ì´Œ', 'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì ì‹¤', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ëŒ€í•™ë¡œ', 'ì¤‘êµ¬', 'ê²½ê¸°', 'ê³ ì–‘', 'ê´‘ì£¼', 'êµ¬ë¦¬', 'êµ°í¬', 'ê¹€í¬', 'ë™íƒ„', 'ë¶€ì²œ', 'ì„±ë‚¨', 'ìˆ˜ì›', 'ì‹œí¥', 'ì•ˆì‚°', 'ì•ˆì–‘', 'ìš©ì¸', 'ì˜ì •ë¶€', 'ì´ì²œ', 'ì¼ì‚°', 'í‰íƒ', 'í•˜ë‚¨', 'í™”ì„±', 'ì¸ì²œ', 'ë‚¨ë™êµ¬', 'ë¯¸ì¶”í™€êµ¬', 'ë¶€í‰êµ¬', 'ì—°ìˆ˜êµ¬', 'ì „ë¶', 'êµ°ì‚°', 'ìµì‚°', 'ì „ì£¼', 'ì¶©ë‚¨', 'ë‹¹ì§„', 'ì²œì•ˆ', 'ê²½ë‚¨', 'ì–‘ì‚°', 'ì§„ì£¼', 'ì°½ì›', 'ê°•ì›', 'ê°•ë¦‰', 'ì›ì£¼', 'ì¶˜ì²œ', 'ì œì£¼', 'ì„œê·€í¬ì‹œ', 'ì œì£¼ì‹œ', 'ì¶©ë¶', 'ì²­ì£¼', 'ì „ë‚¨', 'ëª©í¬', 'ìˆœì²œ', 'ì—¬ìˆ˜', 'ê²½ë¶', 'ê²½ì£¼', 'êµ¬ë¯¸', 'ì˜ì£¼', 'í¬í•­'

**ì—”í‹°í‹° ì¶”ì¶œ ì˜ˆì‹œ**:
- "ê°•ë‚¨ì—ì„œ ì¶”ë¦¬ í…Œë§ˆë¡œ ì¶”ì²œí•´ì¤˜" â†’ {{"preferred_region": ["ê°•ë‚¨"], "preferred_themes": ["ì¶”ë¦¬"]}}
- "ê³µí¬ í…Œë§ˆëŠ” ì ˆëŒ€ ì•ˆë¼" â†’ {{"excluded_themes": ["í˜¸ëŸ¬/ê³µí¬"]}}
- "í˜¸ëŸ¬ ì‹«ì–´, ìŠ¤ë¦´ëŸ¬ë„ ì‹«ì–´" â†’ {{"excluded_themes": ["í˜¸ëŸ¬/ê³µí¬", "ìŠ¤ë¦´ëŸ¬"]}}
- "2ëª…ì´ í•  ìˆ˜ ìˆëŠ” ê±°" â†’ {{"group_size": 2}}
- "ë‚¨ìì¹œêµ¬ë‘ í• ë§Œí•œ" â†’ {{"relationship": "ì»¤í”Œ", "preferred_themes": ["ë¡œë§¨ìŠ¤", "ë“œë¼ë§ˆ"]}}
- "ê°•ë‚¨ì—ì„œ ë‚¨ìì¹œêµ¬ë‘ í• ë§Œí•œ í…Œë§ˆ ì¶”ì²œí•´ì¤˜. ê³µí¬ í…Œë§ˆëŠ” ì ˆëŒ€ ì•ˆë¼" â†’ {{"preferred_region": ["ê°•ë‚¨"], "relationship": "ì»¤í”Œ", "preferred_themes": ["ë¡œë§¨ìŠ¤"], "excluded_themes": ["í˜¸ëŸ¬/ê³µí¬"]}}

**ì‘ë‹µ í˜•ì‹**:
JSON ìµœìƒìœ„ì— "_meta" ê°ì²´ë¥¼ í¬í•¨í•˜ì—¬ ë²„ì „ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.

JSON ì‘ë‹µ:
"""

def _build_prompt_by_version(version: str, user_message: str) -> str:
    """í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ë¶„ê¸°"""
    if version == "intent.v1.3":
        return _build_prompt_v1_3(user_message)
    else:
        return _build_prompt_v1_2(user_message)  # fallback


class Intent:
    """ì‚¬ìš©ì ì˜ë„ íŒŒì•…"""
    QUESTION = "question" # ì§ˆë¬¸
    RECOMMENDATION = "recommendation" # ì¶”ì²œ ìš”ì²­
    GENERAL_CHAT = "general_chat" # ì¼ë°˜ ëŒ€í™”
    PREFERENCE_CHECK = "preference_check" # ì„ í˜¸ë„ ì²´í¬ / cf. QUESTIONNAIRE = "questionnaire"

async def analyze_intent(user_message: str) -> Dict[str, Any]:
    """í•˜ì´ë¸Œë¦¬ë“œ ì˜ë„ ë¶„ì„: LLM ìš°ì„ , DB fallback"""
    try:
        # 1. LLM ê¸°ë°˜ ì˜ë„ ë¶„ì„ ì‹œë„
        llm_result = await _analyze_intent_with_llm(user_message)
        
        # 2. LLM ê²°ê³¼ê°€ ì‹ ë¢°í•  ë§Œí•˜ë©´ ì‚¬ìš©
        # e.g. {'intent': 'recommendation', 'confidence': 0.85, 'entities': {'ì§€ì—­': 'ê°•ë‚¨', 'í…Œë§ˆ': 'ê³µí¬'}, 'reasoning': 'ì‚¬ìš©ìê°€ ê°•ë‚¨ì—ì„œ ë‚¨ìì¹œêµ¬ì™€ í• ë§Œí•œ í…Œë§ˆë¥¼ ì¶”ì²œí•´ì£¼ê¸¸ ì›í•˜ë©°, ê³µí¬ í…Œë§ˆëŠ” ì ˆëŒ€ ì•ˆëœë‹¤ê³  ëª…ì‹œí•¨'}
        if llm_result.get("confidence", 0) > 0.6:
            logger.info(f"LLM intent analysis successful: {llm_result['intent']}")
            return llm_result
        
        # 3. LLM ì‹¤íŒ¨ ì‹œ DB íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ fallback
        logger.info("LLM analysis failed, falling back to pattern matching")
        return await _analyze_intent_pattern_fallback(user_message)
        
    except Exception as e:
        logger.error(f"Hybrid intent analysis error: {e}")
        return await _analyze_intent_pattern_fallback(user_message)

async def _analyze_intent_with_llm(user_message: str) -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•œ ì˜ë„ ë¶„ì„"""
    try:
        # LLM ì„œë¹„ìŠ¤ì˜ ê¸°ì¡´ LLM ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        
        prompt = _build_prompt_by_version(
            settings.nlp_prompt_version, 
            user_message
        )
        
        # LangChain ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
        response = await llm.llm.agenerate([[HumanMessage(content=prompt)]])
        
        # ì‘ë‹µ ì¶”ì¶œ
        response_text = response.generations[0][0].text.strip()
        
        # JSON íŒŒì‹±
        try:
            intent_data = json.loads(response_text)
            
            # ë©”íƒ€ë°ì´í„° ì£¼ì… (LLMì´ í¬í•¨í•˜ì§€ ì•Šì€ ê²½ìš° ëŒ€ë¹„)
            if "_meta" not in intent_data:
                intent_data["_meta"] = {}
            
            intent_data["_meta"].setdefault("prompt_version", settings.nlp_prompt_version)
            intent_data["_meta"].setdefault("schema_version", settings.nlp_schema_version)
            intent_data["_meta"].setdefault("timestamp", now_korea_iso())
            
            return intent_data
        except json.JSONDecodeError:
            logger.warning("LLM response JSON parsing failed")
            raise Exception("JSON parsing failed")
        
    except Exception as e:
        logger.error(f"LLM intent analysis error: {e}")
        raise e

async def _analyze_intent_pattern_fallback(user_message: str) -> Dict[str, Any]:
    """DB ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­ fallback"""
    message = user_message.lower().strip()
    
    # DBì—ì„œ ì˜ë„ íŒ¨í„´ ì¡°íšŒ
    intent_patterns = await _get_intent_patterns()
    
    best_match = None
    best_confidence = 0.0
    
    # ê° ì˜ë„ë³„ íŒ¨í„´ ë§¤ì¹­
    for intent_name, patterns in intent_patterns.items():
        for pattern_data in patterns:
            pattern = pattern_data['pattern']
            confidence = pattern_data['confidence']
            
            if pattern in message:
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_match = {
                        "intent": intent_name,
                        "confidence": confidence,
                        "reasoning": f"Pattern fallback: '{pattern}'",
                        "method": "pattern_matching"
                    }
    
    # ë§¤ì¹­ëœ ì˜ë„ê°€ ìˆìœ¼ë©´ ë°˜í™˜
    if best_match:
        return best_match
    
    # ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”
    return {
        "intent": Intent.GENERAL_CHAT,
        "confidence": 0.3,
        "reasoning": "No pattern matched - fallback to general chat",
        "method": "fallback_default"
    }

async def _get_intent_patterns() -> Dict[str, List[Dict]]:
    """ì˜ë„ íŒ¨í„´ ì¡°íšŒ (ë¡œê¹… + ì˜ˆì™¸ ì²˜ë¦¬)"""
    try:
        patterns = await get_intent_patterns_from_db()
        logger.info(f"Intent patterns loaded successfully: {len(patterns)} intents")
        return patterns
    except Exception as e:
        logger.error(f"Failed to fetch intent patterns: {e}")
        # Fallback ë°ì´í„° ë°˜í™˜
        fallback_patterns = {
            "recommendation": [
                {"pattern": "ì¶”ì²œ", "confidence": 1.0},
                {"pattern": "ì°¾ì•„", "confidence": 0.9}
            ]
        }
        logger.warning(f"Using fallback intent patterns: {fallback_patterns}")
        return fallback_patterns

# =============================================================================
# ì„ í˜¸ë„ ë‹µë³€ ë¶„ì„ í•¨ìˆ˜ë“¤ (LLM ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬)
# =============================================================================

# PreferenceAnalyzer í´ë˜ìŠ¤ ì œê±°ë¨ - core/llm.pyì˜ llm ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©

async def analyze_experience_answer(user_answer: str) -> str:
    """ì‹¤ë¬´ ìµœì í™”: íŒ¨í„´ ë§¤ì¹­ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: ê°•ë ¥í•œ íŒ¨í„´ ë§¤ì¹­ (95% ì¼€ì´ìŠ¤ ì»¤ë²„)
    experienced_patterns = [
        "í•´ë´¤", "ê²½í—˜", "ê°”ì—ˆ", "í•´ë³¸", "ì˜ˆ", "ë„¤", "ìˆì–´ìš”", "ìˆìŠµë‹ˆë‹¤", 
        "ëª‡ ë²ˆ", "ì—¬ëŸ¬ ë²ˆ", "ìì£¼", "ê°€ë´¤", "í•´ë´¤ì–´", "ê°”ì–´", "í•´ë´¤ìŠµë‹ˆë‹¤"
    ]
    beginner_patterns = [
        "ì²˜ìŒ", "ì•ˆí•´ë´¤", "ëª°ë¼", "ì•„ë‹ˆìš”", "ì•„ë‹ˆ", "ì—†ì–´ìš”", "ì—†ìŠµë‹ˆë‹¤",
        "í•œ ë²ˆë„", "ì „í˜€", "ëª¨ë¦„", "ëª¨ë¥´ê² ", "ì•ˆ ê°”", "ì•ˆ í•´ë´¤"
    ]
    
    user_lower = user_answer.lower()
    
    # ê²½í—˜ ìˆìŒ íŒ¨í„´ í™•ì¸
    for pattern in experienced_patterns:
        if pattern in user_lower:
            return "experienced"
    
    # ê²½í—˜ ì—†ìŒ íŒ¨í„´ í™•ì¸
    for pattern in beginner_patterns:
        if pattern in user_lower:
            return "beginner"
    
    # 2ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        ë°©íƒˆì¶œ ê²½í—˜ ì§ˆë¬¸ ë‹µë³€: "{user_answer}"
        
        experienced ë˜ëŠ” beginner ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip().lower()
        
        return result if result in ["experienced", "beginner"] else "beginner"
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return "beginner"  # ì•ˆì „í•œ ê¸°ë³¸ê°’

async def analyze_experience_count(user_answer: str) -> Dict[str, Any]:
    """ì‹¤ë¬´ ìµœì í™”: ìˆ«ì ì¶”ì¶œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: ìˆ«ì ì§ì ‘ ì¶”ì¶œ 
    numbers = re.findall(r'\d+', user_answer)
    
    if numbers:
        count = int(numbers[0])
        if 1 <= count <= 10:
            return {"count": count, "level": "ë°©ë¦°ì´"}
        elif 11 <= count <= 30:
            return {"count": count, "level": "ë°©ì†Œë…„"}
        elif 31 <= count <= 50:
            return {"count": count, "level": "ë°©ì–´ë¥¸"}
        elif 51 <= count <= 80:
            return {"count": count, "level": "ë°©ì‹ "}
        elif 81 <= count <= 100:
            return {"count": count, "level": "ë°©ì¥ë¡œ"}
        elif count > 100:
            return {"count": count, "level": "ë°©ì¥ë¡œ"}
    
    # 2ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­
    user_lower = user_answer.lower()
    if any(word in user_lower for word in ["ë§ì´", "ìì£¼", "100íšŒ", "ë°±íšŒ"]):
        return {"count": 120, "level": "ë°©ì¥ë¡œ"}
    elif any(word in user_lower for word in ["ì¡°ê¸ˆ", "ëª‡ ë²ˆ", "ì ê²Œ"]):
        return {"count": 5, "level": "ë°©ë¦°ì´"}
    
    # 3ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        ê²½í—˜ íšŸìˆ˜ ë‹µë³€: "{user_answer}"
        
        1-10, 11-30, 31-50, 51-80, 81-100, 100+ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        if result == "1-10":
            return {"count": 5, "level": "ë°©ë¦°ì´"}
        elif result == "11-30":
            return {"count": 20, "level": "ë°©ì†Œë…„"}
        elif result == "31-50":
            return {"count": 40, "level": "ë°©ì–´ë¥¸"}
        elif result == "51-80":
            return {"count": 65, "level": "ë°©ì‹ "}
        elif result == "81-100":
            return {"count": 90, "level": "ë°©ì¥ë¡œ"}
        elif result == "100+":
            return {"count": 120, "level": "ë°©ì¥ë¡œ"}
        else:
            return {"count": 5, "level": "ë°©ë¦°ì´"}  # ì•ˆì „í•œ ê¸°ë³¸ê°’
            
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return {"count": 5, "level": "ë°©ë¦°ì´"}

async def analyze_difficulty_answer(user_answer: str) -> int:
    """ì‹¤ë¬´ ìµœì í™”: ì´ëª¨ì§€/í‚¤ì›Œë“œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: ì´ëª¨ì§€ ê°œìˆ˜ë¡œ ë‚œì´ë„ ê²°ì • (80% ì¼€ì´ìŠ¤)
    difficulty = user_answer.count("ğŸ”’")
    if difficulty > 0:
        return min(difficulty, 3)  # ìµœëŒ€ 3
    
    # 2ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­
    user_lower = user_answer.lower()
    if any(word in user_lower for word in ["ì‰¬ìš´", "ì‰½ê²Œ", "ì´ˆë³´", "1"]):
        return 1
    elif any(word in user_lower for word in ["ì–´ë ¤ìš´", "ì–´ë µê²Œ", "ê³ ìˆ˜", "3"]):
        return 3
    elif any(word in user_lower for word in ["ë³´í†µ", "ì ë‹¹", "2"]):
        return 2
    
    # 3ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        ë‚œì´ë„ ë‹µë³€: "{user_answer}"
        
        1, 2, 3 ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        return int(result) if result in ["1", "2", "3"] else 2
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return 2  # ì•ˆì „í•œ ê¸°ë³¸ê°’

async def analyze_activity_answer(user_answer: str) -> int:
    """ì‹¤ë¬´ ìµœì í™”: í‚¤ì›Œë“œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­ (95% ì¼€ì´ìŠ¤)
    user_lower = user_answer.lower()
    if any(word in user_lower for word in ["ê±°ì˜ ì—†ìŒ", "ì ìŒ", "ì¡°ê¸ˆ", "1"]):
        return 1
    elif any(word in user_lower for word in ["ë§ìŒ", "í™œë°œ", "ë§ì´", "3"]):
        return 3
    elif any(word in user_lower for word in ["ë³´í†µ", "ì ë‹¹", "2"]):
        return 2
    
    # 2ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        í™œë™ì„± ë‹µë³€: "{user_answer}"
        
        1, 2, 3 ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        return int(result) if result in ["1", "2", "3"] else 2
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return 2  # ì•ˆì „í•œ ê¸°ë³¸ê°’

async def analyze_group_size_answer(user_answer: str) -> int:
    """ì‹¤ë¬´ ìµœì í™”: ìˆ«ì ì¶”ì¶œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: ìˆ«ì ì§ì ‘ ì¶”ì¶œ (95% ì¼€ì´ìŠ¤)
    numbers = re.findall(r'\d+', user_answer)
    
        if numbers:
        size = int(numbers[0])
        if 2 <= size <= 10:  # í•©ë¦¬ì ì¸ ë²”ìœ„
            return size
        elif size == 1:
            return 2  # 1ëª…ì€ 2ëª…ìœ¼ë¡œ ì¡°ì •
        elif size > 10:
            return 10  # 10ëª… ì´ìƒì€ 10ëª…ìœ¼ë¡œ ì œí•œ
    
    # 2ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­
    user_lower = user_answer.lower()
    if any(word in user_lower for word in ["ë‘˜ì´", "2ëª…", "ë‘ ëª…"]):
        return 2
    elif any(word in user_lower for word in ["ì…‹ì´", "3ëª…", "ì„¸ ëª…"]):
        return 3
    elif any(word in user_lower for word in ["ë„·ì´", "4ëª…", "ë„¤ ëª…"]):
        return 4
    
    # 3ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        ì¸ì›ìˆ˜ ë‹µë³€: "{user_answer}"
        
        ìˆ«ìë§Œ ë‹µë³€í•˜ì„¸ìš” (ì˜ˆ: 3).
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        size = int(result) if result.isdigit() else 3
        return min(max(size, 2), 10)  # 2-10 ë²”ìœ„ë¡œ ì œí•œ
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return 3  # ì•ˆì „í•œ ê¸°ë³¸ê°’

async def analyze_region_answer(user_answer: str) -> str:
    """ì‹¤ë¬´ ìµœì í™”: í‚¤ì›Œë“œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­ (95% ì¼€ì´ìŠ¤)
    regions = ["ê°•ë‚¨", "í™ëŒ€", "ê±´ëŒ€", "ì‹ ì´Œ", "ê°•ë¶", "ì ì‹¤", "ì†¡íŒŒ", "ë§ˆí¬", "ìš©ì‚°"]
    for region in regions:
        if region in user_answer:
            return region
    
    # 2ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        ì§€ì—­ ë‹µë³€: "{user_answer}"
        
        ê°•ë‚¨, í™ëŒ€, ê±´ëŒ€, ì‹ ì´Œ, ê°•ë¶, ì ì‹¤, ì†¡íŒŒ, ë§ˆí¬, ìš©ì‚°, ê¸°íƒ€ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        return result if result in regions + ["ê¸°íƒ€"] else "ê°•ë‚¨"
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return "ê°•ë‚¨"  # ì•ˆì „í•œ ê¸°ë³¸ê°’

async def analyze_theme_answer(user_answer: str) -> str:
    """ì‹¤ë¬´ ìµœì í™”: í‚¤ì›Œë“œ ìš°ì„ , LLM ìµœì†Œ ì‚¬ìš©"""
    # 1ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­ (95% ì¼€ì´ìŠ¤)
    themes = ["ì¶”ë¦¬", "ê³µí¬", "íŒíƒ€ì§€", "SF", "ìŠ¤ë¦´ëŸ¬", "ëª¨í—˜", "ë¡œë§¨ìŠ¤", "ì½”ë¯¸ë””"]
    for theme in themes:
        if theme in user_answer:
            return theme
    
    # 2ë‹¨ê³„: ë¶ˆë¶„ëª…í•œ ê²½ìš°ë§Œ LLM ì‚¬ìš© (5% ì¼€ì´ìŠ¤)
    try:
        prompt = f"""
        í…Œë§ˆ ë‹µë³€: "{user_answer}"
        
        ì¶”ë¦¬, ê³µí¬, íŒíƒ€ì§€, SF, ìŠ¤ë¦´ëŸ¬, ëª¨í—˜, ë¡œë§¨ìŠ¤, ì½”ë¯¸ë”” ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = await llm.llm.ainvoke(prompt)
        result = response.content.strip()
        
        return result if result in themes else "ì¶”ë¦¬"
        
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return "ì¶”ë¦¬"  # ì•ˆì „í•œ ê¸°ë³¸ê°’